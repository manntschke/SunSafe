{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsfresh data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import make_pipeline as imb_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_processed/client_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns\n",
    "merged_df.drop(\"creation_date\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert invoice_date to datetime\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the target variable and features\n",
    "target = ['target']\n",
    "num_features = ['client_id',\n",
    "                '1transactions_count',\n",
    "                'consumption_level_1_mean',\n",
    "                'consumption_level_2_mean',\n",
    "                'consumption_level_3_mean',\n",
    "                'consumption_level_4_mean']\n",
    "cat_features = [col for col in merged_df.columns if col not in num_features and col not in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n",
    "    ('one_hot_encoding', OneHotEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "    #('num', num_pipeline, num_features)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_df.copy()\n",
    "X = df.drop(columns=[\"target\"], axis=1)  \n",
    "y = df[\"target\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced dataset?\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stage1 = XGBClassifier()\n",
    "                            # n_estimators=500, \n",
    "                            #  random_state=42,\n",
    "                            #  max_depth=5,        # avoids overfitting\n",
    "                            # learning_rate=0.05, # Slow learning\n",
    "                            # subsample=0.8,      # randomness\n",
    "                            # colsample_bytree=0.8, \n",
    "                            # scale_pos_weight=scale_pos_weight,  # Adjust based on fraud ratio\n",
    "                            # eval_metric=\"aucpr\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model_stage1) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=10, sampling_strategy='auto') \n",
    "\n",
    "# pipeline\n",
    "ros_pipeline = imb_pipe(ros, model_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter search space\n",
    "param_distributions = {\n",
    "    'pipeline__model__n_estimators': np.arange(50, 500, 50),\n",
    "    'pipeline__model__max_depth': np.arange(3, 10),\n",
    "    'pipeline__model__learning_rate': np.logspace(-3, 0, 10),\n",
    "    'pipeline__model__subsample': np.linspace(0.5, 1, 5),\n",
    "    'pipeline__model__scale_pos_weight': [0]\n",
    "}\n",
    "\n",
    "# Define RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    ros_pipeline, param_distributions,\n",
    "    n_iter=20,  # Number of random samples\n",
    "    scoring='roc_auc',  # Optimize for F1-score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all CPUs\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the best model\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification report of best model\n",
    "y_pred = best_model.predict(X_test) \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix of best model\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Output predictions\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # probabilities for fraud\n",
    "fraud_probs_stage1 = model_pipeline.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "# Fraud probabilities for second stage model > 50%\n",
    "# 1st stage: optimize recall, 2nd stage optimize precision\n",
    "suspicious_cases = X_test[fraud_probs_stage1 > 0.30] # only use propabilities larger 30% for2nd stage model\n",
    "\n",
    "# not needed \n",
    "y_suspicious = y_test[fraud_probs_stage1 > 0.30] # snot used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree for second stage\n",
    "dt_model_stage2 = LogisticRegression(C=0.1, \n",
    "    penalty='l1', \n",
    "    solver='liblinear', \n",
    "    class_weight='balanced', \n",
    "    max_iter=500)\n",
    "dt_model_stage2.fit(suspicious_cases, y_suspicious)\n",
    "\n",
    "fraud_probs_stage2 = dt_model_stage2.predict_proba(suspicious_cases)[:, 1]\n",
    "\n",
    "# categorize risk\n",
    "def risk_category(prob):\n",
    "    if prob > 0.7:\n",
    "        return \"High Risk\"\n",
    "    elif prob > 0.4:\n",
    "        return \"Medium Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "suspicious_cases_output = suspicious_cases.copy()\n",
    "suspicious_cases_output[\"Risk Level\"] = [risk_category(p) for p in fraud_probs_stage2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_risk_cases = suspicious_cases_output#[suspicious_cases_output[\"Risk Level\"] == \"High Risk\"]\n",
    "high_risk_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot high risk for each risk level barplot count\n",
    "high_risk_cases[\"Risk Level\"].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
