{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from joblib import dump, load\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load baseline\n",
    "# choose if u want to look at baseline model:\n",
    "# with open('models/baselinemodel_data.pkl', 'rb') as f:\n",
    "#     X_train, X_test, y_train, y_test, basemodel = pickle.load(f)\n",
    "# basemodel\n",
    "\n",
    "# load stoneage\n",
    "loaded_data = load('models/model_stoneage.pkl')\n",
    "best_model, preprocessor, X_train, X_test, y_train, y_test, model_stoneage = (\n",
    "loaded_data['best_model']  ,  \n",
    "loaded_data['preprocessor'] ,\n",
    "loaded_data[\"X_train\"], \n",
    "loaded_data[\"X_test\"],\n",
    "loaded_data[\"y_train\"],\n",
    "loaded_data[\"y_test\"], \n",
    "loaded_data[\"model\"])\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use preprocessor to transform the data\n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best model from e.e. gridsearch ( stroed on pkl file)\n",
    "\n",
    "# for baseline activate this cell, else dont\n",
    "#best_model = basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {type(best_model).__name__}\\n\")\n",
    "print(\"Model: parameters\\n\")\n",
    "for param, value in best_model.get_params().items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "    \n",
    "# add crossvalidation score here to evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation set later (maybe somewhere else)\n",
    "# X_validation = pd.read_csv('data/data_processed/validation.csv')\n",
    "# y_pred_validation = basemodel.predict(X_validation)\n",
    "# y_pred_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict y on train data\n",
    "\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "# predict y on train data\n",
    "\n",
    "\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# performance metrics\n",
    "# confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Train Data Confusion Matrix')\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=ax[1])\n",
    "ax[1].set_title('Test Data Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report training data\n",
    "print(\"Training data results:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "#classification report test data\n",
    "print(\"Test data results\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "# print differnce between accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_diff = abs(train_accuracy - test_accuracy)\n",
    "\n",
    "print(\"Difference in accuracy Train-Test is: {:.2%}\".format(accuracy_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc\n",
    "auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(f\"Train AUC: {auc_train}\")\n",
    "print(f\"Test AUC: {auc_test}\")\n",
    "# differences in auc values\n",
    "auc_diff = abs(auc_train - auc_test)\n",
    "print(f\"Difference in AUC values: {auc_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc auc curve plot\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot ROC curve for training data\n",
    "ax[0].plot(fpr_train, tpr_train, label=f'Train AUC: {auc_train:.2f}')\n",
    "ax[0].plot([0, 1], [0, 1], 'k--')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive Rate')\n",
    "ax[0].set_title('ROC Curve - Training Data')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# Plot ROC curve for test data\n",
    "ax[1].plot(fpr_test, tpr_test, label=f'Test AUC: {auc_test:.2f}')\n",
    "ax[1].plot([0, 1], [0, 1], 'k--')\n",
    "ax[1].set_xlabel('False Positive Rate')\n",
    "ax[1].set_ylabel('True Positive Rate')\n",
    "ax[1].set_title('ROC Curve - Test Data')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation score\n",
    "# once we have one, we can ckeck it here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
